{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "프로그램 도입부"
      ],
      "metadata": {
        "id": "TuQ3fNPD_SM9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_DZoXhAmgBer"
      },
      "outputs": [],
      "source": [
        "## frozen-lake 문제에 대한 DQN 프로그램.\n",
        "##\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "import math\n",
        "from datetime import datetime\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "# if GPU is to be used\n",
        "#device = torch.device(\"cpu\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "total_episodes = 60000 # Total number of episodes in training\n",
        "max_steps = 99 # Max steps per episode in training.\n",
        "gamma = 0.90 # Discounting rate for expected return\n",
        "Learning_rate = 0.00005 # 신경망 모델 learning rate (optimizer 에게 제공)\n",
        "original_epsilon = 0.4 # Exploration rate\n",
        "decay_rate = 0.000006 # Exponential decay rate for exploration.\n",
        "TAU = 0.7 # Q_net 파라메터를 Q_hat_net 로 copy 시에 반영 비율.\n",
        "one_minus_TAU = 1 - TAU\n",
        "\n",
        "memory_pos = 0 # replay_memory 내에 transition 을 넣을 다음 위치.\n",
        "# 0 에서 부터 커지다가 max_memory-1 까지 되면 다시 0 부터 시작함.\n",
        "BATCH_SIZE = 16\n",
        "model_update_cnt = 0 # Q_net 를 업데이트한 횟수.\n",
        "copy_cnt = 4 # Q_net 업데이트를 copy_cnt 번 한 후마다 Q_hat_net 로 파라메터 복사.\n",
        "max_memory = 2000 # capacity of the replay memory.\n",
        "transition_cnt = 0 # 거쳐간 총 transition 수(episodes 간에 중단 없이)\n",
        "# 매 (배치크기+작은 랜덤값) 마다 Q_net 의 parameter update 를 수행함.\n",
        "random.seed(datetime.now().timestamp()) # give a new seed in random number generation.\n",
        "# state space is defined as size_row X size_col array.\n",
        "# The boundary cells are holes(H).\n",
        "# S: start, G: goal, H:hole, F:frozen\n",
        "max_row = 9\n",
        "max_col = 9\n",
        "n_actions = 4 # 0:up, 1:right, 2:down, 3:left.\n",
        "n_observations = max_row * max_col # total number of states\n",
        "# 1-hot 벡터로 표현하므로 NN 입력의 신호수 = 총 state 수\n",
        "env_state_space = \\\n",
        "  [['H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H'], \\\n",
        "   ['H', 'S', 'F', 'F', 'F', 'F', 'F', 'F', 'H'], \\\n",
        "   ['H', 'F', 'F', 'H', 'H', 'F', 'H', 'F', 'H'], \\\n",
        "\t ['H', 'F', 'F', 'F', 'F', 'F', 'F', 'H', 'H'], \\\n",
        "\t ['H', 'F', 'H', 'F', 'H', 'F', 'F', 'H', 'H'], \\\n",
        "\t ['H', 'F', 'F', 'F', 'F', 'G', 'F', 'F', 'H'], \\\n",
        "   ['H', 'F', 'H', 'H', 'F', 'H', 'F', 'F', 'H'], \\\n",
        "   ['H', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'H'], \\\n",
        "   ['H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H']]\n",
        "\n",
        "# offset of each move action: up, right, down, left, respectively.\n",
        "# a new state(location) = current state + offset of an action.\n",
        "move_offset = [[-1,0], [0,1], [1,0], [0,-1]]\n",
        "move_str = ['up ', 'right', 'down ', 'left ']\n",
        "# replay memory: transition 들을 저장하는 버퍼.\n",
        "# 저장되는 transition 의 4가지 정보: state_index, action, reward, next state index.\n",
        "# (주의: state 를 좌표 대신 상태번호(index) 로 나타냄.)\n",
        "replay_memory = np.ndarray((max_memory, 4), dtype=int)\n",
        "batch_transition = np.ndarray((BATCH_SIZE, 4), dtype=int) # 배치 하나를 넣는데 사용.\n",
        "is_replay_memory_full = 0 # 버퍼가 처음으로 완전히 채워지기 전에는 0. 그후로는 항상 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습 후 DQN 을 이용하여 q values 를 출력해 보는 데 이용하는 함수"
      ],
      "metadata": {
        "id": "Vm89p_g6_Ga_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_and_print_Q_values (s):\n",
        "  r = s[0]\n",
        "  c = s[1]\n",
        "  if env_state_space[r][c] == 'G' or env_state_space[r][c] == 'H':\n",
        "    action_values = [ 0.0 for i in range(n_actions)]\n",
        "    action_values = np.array(action_values)\n",
        "  else:\n",
        "    state_idx = r * max_col + c # state 의 번호를 만듬.\n",
        "    state_idx_list = [state_idx] # 배치 차원을 넣는다. 배치는 하나의 예제 입력만 가짐.\n",
        "    states_tsr = torch.tensor(state_idx_list).to(device) # state 한개 가짐\n",
        "    one_hot_states_tsr = F.one_hot(states_tsr, num_classes= n_observations)\n",
        "    one_hot_states_tsr = one_hot_states_tsr.float().to(device)\n",
        "    with torch.no_grad():\n",
        "      state_action_values = Q_net(one_hot_states_tsr) # 주의: 출력은 2차원: (1, n_actions).\n",
        "      state_action_values = state_action_values[0] # 배치 차원을 없앤다.\n",
        "      action_values = state_action_values.cpu().numpy()\n",
        "\n",
        "  text = \"s[\" + str(r) + \",\" + str(c) + \"]: \"\n",
        "  for i in range(n_actions):\n",
        "    #text = text + str(action_values[i]) + \", \"\n",
        "    text = text + \"{:5.2f}\".format(action_values[i])+ \", \"\n",
        "  print(text)\n",
        "\n",
        "# state s 에서 greedy 하게 action 을 고른다.\n",
        "def choose_action_with_greedy(s):\n",
        "  state_idx = s[0] * max_col + s[1] # state 의 표현을 상태번호로 바꾼다.\n",
        "  state_idx_list = [state_idx] # 1-차원 데이터임.배치 차원을 넣은 것임.\n",
        "  states_tsr = torch.tensor(state_idx_list).to(device) # state 하나를 가지는 배열.\n",
        "  one_hot_states_tsr = F.one_hot(states_tsr, num_classes= n_observations)\n",
        "  one_hot_states_tsr = one_hot_states_tsr.float().to(device)\n",
        "\n",
        "  # get q-a values of all actions for state s.\n",
        "  with torch.no_grad():\n",
        "    state_action_values = Q_net(one_hot_states_tsr) # 출력은 2차원: (1, n_actions).\n",
        "\n",
        "  max_a = torch.argmax(state_action_values, dim=1) # 값이 최대인 액션 번호를 얻는다.\n",
        "  max_a = max_a[0] # 입력 하나에 대한 결과를 가지는 리스트에서 액션 하나를 꺼냄.\n",
        "  return max_a\n",
        "  ## end def choose_action_with_greedy(s):\n",
        "\n",
        "# state s 에서 epsilon-greedy 방식으로 다음 action 을 고른다.\n",
        "def choose_action_with_epsilon_greedy(s, epsilon):\n",
        "  state_idx = s[0] * max_col + s[1] # state 의 표현을 상태번호로 바꾼다.\n",
        "  state_idx_list = [state_idx] # 배치 차원을 넣어 줌. 배치에 1 개만 가짐.\n",
        "  states_tsr = torch.tensor(state_idx_list).to(device) # state 한 개 가짐\n",
        "  one_hot_states_tsr = F.one_hot(states_tsr, num_classes= n_observations)\n",
        "  one_hot_states_tsr = one_hot_states_tsr.float().to(device)\n",
        "  # get q-a values of all actions of a state in batch having one state.\n",
        "  with torch.no_grad():\n",
        "    state_action_values = Q_net(one_hot_states_tsr) # 주의: 출력은 2차원: (bsz, n_actions).\n",
        "  max_a = torch.argmax(state_action_values, dim=1).item() # 값이 최대인 액션.\n",
        " # max_a = max_a[0]\n",
        "\n",
        "  rn = random.random() # 0 ~ 1 사이 random number.\n",
        "  if rn >= epsilon: # epsilon 보다 크면, 최대확률을 가진 action 을 선택.\n",
        "    action = max_a\n",
        "  else:\n",
        "     action = random.randint(0, 3)  # 무작위 액션 선택\n",
        "  return action\n",
        "\n",
        "'''\n",
        "    rn1 = random.random()\n",
        "  # 4 개의 action 중 하나를 무작위로 선택.\n",
        "    if rn1 >= 0.75:\n",
        "      action = 0\n",
        "    elif rn1 >= 0.5:\n",
        "      action = 1\n",
        "    elif rn1 >= 0.25:\n",
        "      action = 2\n",
        "    else:\n",
        "      action = 3\n",
        " '''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "ZfJSuQUw_ZSr",
        "outputId": "4a2d134e-1f7e-44f4-a1cc-5daec968ee2f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n    rn1 = random.random()\\n  # 4 개의 action 중 하나를 무작위로 선택.\\n    if rn1 >= 0.75:\\n      action = 0\\n    elif rn1 >= 0.5:\\n      action = 1\\n    elif rn1 >= 0.25:\\n      action = 2\\n    else:\\n      action = 3\\n '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "신경망 모델 정의 함수\n"
      ],
      "metadata": {
        "id": "CxsBmlLq_edz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 정의: Function approximation 에 사용할 신경망 모델 구조를 정의한다.\n",
        "\n",
        "class DQN(nn.Module):\n",
        "  def __init__(self, n_observations, n_actions):\n",
        "    super(DQN, self).__init__()\n",
        "    self.layer1 = nn.Linear(n_observations, 128)\n",
        "    self.layer2 = nn.Linear(128, 128)\n",
        "    self.layer3 = nn.Linear(128, n_actions)\n",
        "\n",
        "  # x 는 2차원 데이터: dim0: batch, dim1: state를 나타내는 1-hot 입력벡터.\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer1(x))\n",
        "    x = F.relu(self.layer2(x))\n",
        "    return self.layer3(x)\n"
      ],
      "metadata": {
        "id": "sxr4AawH_f4B"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "배치 하나를 이용하여 학습을 수행하는 함수: learning_by_a_batch\n"
      ],
      "metadata": {
        "id": "S6bYxudf_iD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def learning_by_a_batch():\n",
        "    state_batch = batch_transition[:, 0]\n",
        "    action_batch = batch_transition[:, 1]\n",
        "    reward_batch = batch_transition[:, 2]\n",
        "    next_state_batch = batch_transition[:, 3]\n",
        "\n",
        "    # Q_net 신경망의 입력층에 배치 데이터 준비. state를 1-hot 벡터로 표시함.\n",
        "    state_batch_tsr = torch.from_numpy(state_batch).to(device)\n",
        "    one_hot_state_batch = F.one_hot(state_batch_tsr, num_classes=n_observations)\n",
        "    one_hot_state_batch = one_hot_state_batch.float().to(device)\n",
        "\n",
        "    #Q_net의 출력은 state마다에 대한 여러 action들의 q(s,a)값이다.\n",
        "    prediction_Q_net = Q_net(one_hot_state_batch) #출력의 shape : (bsz, n_actions)\n",
        "\n",
        "    state_action_values_tsr = torch.zeros([BATCH_SIZE], dtype = torch.float64).to(device)\n",
        "    for i in range(BATCH_SIZE):\n",
        "      action_index = action_batch[i]\n",
        "\n",
        "        # 액션 인덱스가 유효한 범위 내에 있는지 확인\n",
        "      if action_index >= prediction_Q_net.shape[1]:\n",
        "        print(f\"Invalid action index {action_index} at batch index {i}.\")\n",
        "        action_index = prediction_Q_net.shape[1] - 1  # 유효한 최대 인덱스로 대체\n",
        "\n",
        "      state_action_values_tsr[i] = prediction_Q_net[i, action_batch[i]]\n",
        "    #shape of state_action_values: (batch_size)\n",
        "\n",
        "    #s'이 next state일 때 max_a{q(s', a)}를 구하자. (batch 내의 모든 transition 마다)\n",
        "    with torch.no_grad():\n",
        "      next_state_batch_tsr = torch.from_numpy(next_state_batch)\n",
        "      one_hot_next_state_batch = F.one_hot(next_state_batch_tsr, num_classes=n_observations)\n",
        "      one_hot_next_state_batch = one_hot_next_state_batch.float().to(device)\n",
        "      result_target_net = Q_hat_net(one_hot_next_state_batch)\n",
        "        ##shape of result_target_net : (bsz, 4)\n",
        "\n",
        "      max_q_of_next_states_in_batch = torch.max(result_target_net, dim=1).values\n",
        "        #주의: .max 함수가 2가지를 출력하므로 .values를 이용함. 결과적으로 출력의  shape:(bsz)\n",
        "    next_state_values = []\n",
        "    for i, st in enumerate(next_state_batch):\n",
        "      r = int(st / max_col)\n",
        "      c = st % max_col\n",
        "      if env_state_space[r][c] == 'G' or env_state_space[r][c] == 'H':\n",
        "        next_state_values.append(0)  # terminal state의 q(s',a) value는 0\n",
        "      else:\n",
        "        # non-terminal state는 게산된 max_a(q(s',a))\n",
        "        next_state_values.append(max_q_of_next_states_in_batch[i].item())\n",
        "\n",
        "    next_state_values_tsr = torch.tensor(next_state_values).to(device)\n",
        "\n",
        "    #update target를 준비한다 : R + gamma*max_a{q(s',a)}\n",
        "    reward_batch_tsr = torch.from_numpy(reward_batch).to(device)\n",
        "    target_state_action_values_tsr = (next_state_values_tsr * gamma) + reward_batch_tsr\n",
        "\n",
        "    #Huber loss로 loss를 계산한다.\n",
        "    loss = criterion(state_action_values_tsr, target_state_action_values_tsr)\n",
        "\n",
        "    optimizer.zero_grad() #parameters의 gradient를 0으로 초기화\n",
        "\n",
        "    #backward computation: 모든 parameter의 gradient를 구한다.\n",
        "    loss.backward()\n",
        "\n",
        "    #In-place gradient clipping\n",
        "    torch.nn.utils.clip_grad_value_(Q_net.parameters(), 100)\n",
        "\n",
        "    #모델의 모든 parameters를 loss의 gradient를 이용하여 update한다.\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "kuxqGoxb_jYA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "두 개의 신경망모델, optimizer, criterion 함수 설정\n"
      ],
      "metadata": {
        "id": "VTiL5J58E716"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 인공신경망 모델 두 개를 만든다:\n",
        "# Q_net: policy를 나타내는 main 신경망모델(policy net of prediction net이라고 부름)\n",
        "# Q_hat_net: target값을 생성하는 신경망모델(target net이라고 부름)\n",
        "\n",
        "Q_net = DQN(n_observations, n_actions).to(device)\n",
        "Q_hat_net = DQN(n_observations, n_actions).to(device)\n",
        "\n",
        "#target_net에 policy_net의 파라메터를 복사해서 완전히 같은 모델로 초기화함\n",
        "Q_hat_net.load_state_dict(Q_net.state_dict())\n",
        "\n",
        "optimizer = optim.AdamW(Q_net.parameters(), lr = Learning_rate, amsgrad = True)\n",
        "criterion = nn.SmoothL1Loss()"
      ],
      "metadata": {
        "id": "1d7MnigIADne"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get new state and reward for taking action a at state s.\n",
        "# deterministic movement is taken.\n",
        "# reward is given as: F/S:0;  H:-5;   G:5.\n",
        "def get_new_state_and_reward(s, a):\n",
        "   new_state = []\n",
        "   off_set = move_offset[a]\n",
        "\n",
        "   #  s + off_set gives the new_state.\n",
        "   new_state.append(s[0] + off_set[0])\n",
        "   new_state.append(s[1] + off_set[1])\n",
        "\n",
        "   # 경계 조건 확인\n",
        "   if new_state[0] < 0 or new_state[0] >= max_row or new_state[1] < 0 or new_state[1] >= max_col:\n",
        "    return s, -1  # 경계를 벗어나면 상태를 변경하지 않고 패널티 부여\n",
        "\n",
        "   # compute reward for moving to the new state\n",
        "   cell = env_state_space[new_state[0]][new_state[1]]\n",
        "   if cell == 'F':\n",
        "      rew = 0\n",
        "   elif cell == 'H':\n",
        "      rew = -9\n",
        "   elif cell == 'G':\n",
        "      rew = 9\n",
        "   elif cell == 'S':\n",
        "      rew = 0\n",
        "   else:\n",
        "      print(\"Logic error in get_new_state_and_reward. This cannot happen!\")\n",
        "      time.sleep(1200)\n",
        "      return [0,0], -20000\n",
        "\n",
        "   return new_state, rew\n"
      ],
      "metadata": {
        "id": "gAQvUWT8kCTQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습단계"
      ],
      "metadata": {
        "id": "u_Mv_sGOFzyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################\n",
        "#   (1) 학습단계\n",
        "################################################################\n",
        "if torch.cuda.is_available():\n",
        "  num_episodes = total_episodes\n",
        "else:\n",
        "  num_episodes = 200\n",
        "\n",
        "start_state = [1,1]\n",
        "print(\"\\n학습단계 시작.\\n\")\n",
        "\n",
        "for i_episode in range(num_episodes):\n",
        "  #get starting state\n",
        "  S = start_state\n",
        "\n",
        "  epsilon = original_epsilon * math.exp(-decay_rate * i_episode) #epsilon 약간 감소시킴\n",
        "\n",
        "  if i_episode != 0 and i_episode % (num_episodes if num_episodes < 4000 else 4000) == 0:\n",
        "    print('episode=', i_episode, ' epsilon=', epsilon)\n",
        "    time.sleep(1)\n",
        "  for t in range(max_steps):\n",
        "\n",
        "    A = choose_action_with_epsilon_greedy(S, epsilon)\n",
        "\n",
        "    # take action A to observe reward R, and new state S_.\n",
        "    S_ , R = get_new_state_and_reward(S, A)\n",
        "\n",
        "    #transposition 하나를 replay memory에 저장\n",
        "    s_idx = S[0] * max_col + S[1]\n",
        "    next_s_idx = S_[0] * max_col + S_[1]\n",
        "\n",
        "    replay_memory[memory_pos, 0] = s_idx\n",
        "    replay_memory[memory_pos, 1]= A\n",
        "    replay_memory[memory_pos, 2] = R\n",
        "    replay_memory[memory_pos, 3] = next_s_idx\n",
        "\n",
        "    #replay memory 버퍼가 처음으로 완전히 차면, is_replay_memory_full에 1을 넣는다\n",
        "    if is_replay_memory_full == 0 and memory_pos == max_memory-1:\n",
        "      is_replay_memory_full = 1\n",
        "\n",
        "    #다음 번에 넣을 위치를 정해 놓는다.\n",
        "    memory_pos = (memory_pos + 1) % max_memory\n",
        "\n",
        "    # Move to the next state\n",
        "    S = S_\n",
        "\n",
        "    # replay_memory로 보낸 총 transition 총 개수\n",
        "    transition_cnt += 1\n",
        "\n",
        "    random_number = random.randint(0, int(BATCH_SIZE/2))\n",
        "\n",
        "    if transition_cnt >= (BATCH_SIZE +3) and transition_cnt % (BATCH_SIZE+random_number) == 0:\n",
        "\n",
        "      ### transition들을 가져와서 배치 1개를 만든다. 결과는 batch_transition에 있다 ####\n",
        "\n",
        "      #replay_memory에서 꺼내올 위치들을 random으로 선정하여 random_number 리스트에 넣는다.\n",
        "      if is_replay_memory_full == 1:\n",
        "      # 전체 영역에서 가져옴\n",
        "        random_numbers = random.sample(range(0, max_memory), BATCH_SIZE)\n",
        "      else:\n",
        "      #아직 버퍼가 완전히 차지 않은 상태임. 버퍼의 채워져있는 부분에서 가져옴.\n",
        "        random_numbers = random.sample(range(0, memory_pos-1), BATCH_SIZE)\n",
        "\n",
        "      #repaly_mememory에서 transition들을 꺼내 와서 배치 하나를 batch_transition에 준비한다.\n",
        "      for i in range(BATCH_SIZE):\n",
        "        rnum = random_numbers[i]\n",
        "        batch_transition[i, :] = replay_memory[rnum,:]\n",
        "      ############################# batch_transition에 배치준비 완료 #######################\n",
        "\n",
        "      #배치 하나를 이용하여 모델을 훈련(parameter updating)시킨다.\n",
        "      learning_by_a_batch()\n",
        "\n",
        "      model_update_cnt += 1  #모델이 update된 총 횟수\n",
        "\n",
        "      #Q_net의 parameter update를 여러번(copy_cnt번) 수행한 후에 Q_hat_net의 parameter를 복사해온다.\n",
        "      if model_update_cnt % copy_cnt == 0:\n",
        "\n",
        "        #soft 복사 사용: Q_net의 parameter값을 일부만 복사함(복사비율: TAU)\n",
        "        Q_hat_net_state_dict = Q_hat_net.state_dict()\n",
        "        Q_net_state_dict = Q_net.state_dict()\n",
        "        for key in Q_net_state_dict:\n",
        "          Q_hat_net_state_dict[key] = Q_net_state_dict[key]*TAU + Q_hat_net_state_dict[key] * one_minus_TAU\n",
        "        Q_hat_net.load_state_dict(Q_hat_net_state_dict)\n",
        "\n",
        "      #terminal state에 도달하면 episode를 종료한다.\n",
        "      if env_state_space[S[0]][S[1]] == 'G' or env_state_space[S[0]][S[1]] == 'H':\n",
        "        break\n",
        "\n",
        "print(\"학습단계 종료\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "075CPnXGF1FR",
        "outputId": "618c7a90-f284-431e-bfa9-e94145973d20"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "학습단계 시작.\n",
            "\n",
            "episode= 4000  epsilon= 0.39051428390316373\n",
            "episode= 8000  epsilon= 0.3812535148310019\n",
            "episode= 12000  epsilon= 0.3722123583244823\n",
            "episode= 16000  epsilon= 0.3633856064274825\n",
            "episode= 20000  epsilon= 0.354768174686863\n",
            "episode= 24000  epsilon= 0.346355099223682\n",
            "episode= 28000  epsilon= 0.33814153387386353\n",
            "episode= 32000  epsilon= 0.33012274739667297\n",
            "episode= 36000  epsilon= 0.3222941207493919\n",
            "episode= 40000  epsilon= 0.31465114442662134\n",
            "episode= 44000  epsilon= 0.30718941586268245\n",
            "episode= 48000  epsilon= 0.2999046368956165\n",
            "episode= 52000  epsilon= 0.29279261129132506\n",
            "episode= 56000  epsilon= 0.2858492423264229\n",
            "학습단계 종료\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "테스트 단계"
      ],
      "metadata": {
        "id": "abMqtoioLlHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"테스트 단계 시작\\n\")\n",
        "\n",
        "for e in range(4):\n",
        "  S = start_state\n",
        "  total_reward = 0\n",
        "  print(\"\\nEpisode=\", e, \"   start state: (\",S[0], \",\", S[1], \")\")\n",
        "  leng = 0\n",
        "  for i in range(99):\n",
        "    A = choose_action_with_greedy(S)\n",
        "    S_, R = get_new_state_and_reward(S,A)\n",
        "    print(\"the move is\", move_str[A], \" to (\", S_[0], \",\", S_[1], \")\")\n",
        "    leng += 1\n",
        "    total_reward += R\n",
        "    S = S_\n",
        "    if env_state_space[S[0]][S[1]] == 'G' or env_state_space[S[0]][S[1]] == 'H':\n",
        "            break\n",
        "  print(\"episode ends. episode length = \", leng, \". total reward = \", total_reward)\n",
        "\n",
        "print(\"테스트단계 종료\")\n",
        "\n",
        "#모든 state_action pair들의 q값을 Q_net를 이용하여 출력하여본다.\n",
        "print(\"\\n학습 후의 Q_values: \")\n",
        "for i in range(max_row):\n",
        "  for j in range(max_col):\n",
        "    s = [i,j] # a state.\n",
        "    compute_and_print_Q_values(s)\n",
        "\n",
        "print(\"프로그램 종료!\")"
      ],
      "metadata": {
        "id": "_aRL7WIyLmf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "344c2b93-fd52-489d-fec2-9172ed6e8767"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "테스트 단계 시작\n",
            "\n",
            "\n",
            "Episode= 0    start state: ( 1 , 1 )\n",
            "the move is down   to ( 2 , 1 )\n",
            "the move is down   to ( 3 , 1 )\n",
            "the move is down   to ( 4 , 1 )\n",
            "the move is down   to ( 5 , 1 )\n",
            "the move is down   to ( 6 , 1 )\n",
            "the move is down   to ( 7 , 1 )\n",
            "the move is right  to ( 7 , 2 )\n",
            "the move is down   to ( 8 , 2 )\n",
            "episode ends. episode length =  8 . total reward =  -9\n",
            "\n",
            "Episode= 1    start state: ( 1 , 1 )\n",
            "the move is down   to ( 2 , 1 )\n",
            "the move is down   to ( 3 , 1 )\n",
            "the move is down   to ( 4 , 1 )\n",
            "the move is down   to ( 5 , 1 )\n",
            "the move is down   to ( 6 , 1 )\n",
            "the move is down   to ( 7 , 1 )\n",
            "the move is right  to ( 7 , 2 )\n",
            "the move is down   to ( 8 , 2 )\n",
            "episode ends. episode length =  8 . total reward =  -9\n",
            "\n",
            "Episode= 2    start state: ( 1 , 1 )\n",
            "the move is down   to ( 2 , 1 )\n",
            "the move is down   to ( 3 , 1 )\n",
            "the move is down   to ( 4 , 1 )\n",
            "the move is down   to ( 5 , 1 )\n",
            "the move is down   to ( 6 , 1 )\n",
            "the move is down   to ( 7 , 1 )\n",
            "the move is right  to ( 7 , 2 )\n",
            "the move is down   to ( 8 , 2 )\n",
            "episode ends. episode length =  8 . total reward =  -9\n",
            "\n",
            "Episode= 3    start state: ( 1 , 1 )\n",
            "the move is down   to ( 2 , 1 )\n",
            "the move is down   to ( 3 , 1 )\n",
            "the move is down   to ( 4 , 1 )\n",
            "the move is down   to ( 5 , 1 )\n",
            "the move is down   to ( 6 , 1 )\n",
            "the move is down   to ( 7 , 1 )\n",
            "the move is right  to ( 7 , 2 )\n",
            "the move is down   to ( 8 , 2 )\n",
            "episode ends. episode length =  8 . total reward =  -9\n",
            "테스트단계 종료\n",
            "\n",
            "학습 후의 Q_values: \n",
            "s[0,0]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[0,1]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[0,2]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[0,3]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[0,4]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[0,5]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[0,6]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[0,7]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[0,8]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[1,0]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[1,1]: -0.03,  0.05,  0.08, -0.06, \n",
            "s[1,2]: -0.04,  0.02,  0.07, -0.07, \n",
            "s[1,3]: -0.03,  0.06,  0.07, -0.06, \n",
            "s[1,4]: -0.03,  0.06,  0.08, -0.05, \n",
            "s[1,5]: -0.03,  0.04,  0.09, -0.07, \n",
            "s[1,6]: -0.04,  0.05,  0.07, -0.08, \n",
            "s[1,7]: -0.04,  0.06,  0.08, -0.07, \n",
            "s[1,8]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[2,0]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[2,1]: -0.04,  0.04,  0.08, -0.07, \n",
            "s[2,2]: -0.03,  0.04,  0.08, -0.07, \n",
            "s[2,3]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[2,4]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[2,5]: -0.02,  0.05,  0.10, -0.06, \n",
            "s[2,6]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[2,7]: -0.04,  0.05,  0.08, -0.06, \n",
            "s[2,8]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[3,0]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[3,1]: -0.02,  0.05,  0.09, -0.06, \n",
            "s[3,2]: -0.03,  0.05,  0.09, -0.07, \n",
            "s[3,3]: -0.03,  0.06,  0.06, -0.06, \n",
            "s[3,4]: -0.02,  0.03,  0.06, -0.06, \n",
            "s[3,5]: -0.02,  0.03,  0.08, -0.06, \n",
            "s[3,6]: -0.02,  0.04,  0.08, -0.06, \n",
            "s[3,7]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[3,8]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[4,0]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[4,1]: -0.02,  0.05,  0.07, -0.06, \n",
            "s[4,2]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[4,3]: -0.04,  0.05,  0.08, -0.05, \n",
            "s[4,4]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[4,5]: -0.01,  0.06,  0.09, -0.06, \n",
            "s[4,6]: -0.02,  0.05,  0.07, -0.06, \n",
            "s[4,7]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[4,8]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[5,0]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[5,1]: -0.05,  0.05,  0.08, -0.06, \n",
            "s[5,2]: -0.02,  0.02,  0.08, -0.05, \n",
            "s[5,3]: -0.04,  0.06,  0.09, -0.05, \n",
            "s[5,4]: -0.02,  0.06,  0.08, -0.06, \n",
            "s[5,5]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[5,6]: -0.01,  0.03,  0.07, -0.06, \n",
            "s[5,7]: -0.03,  0.06,  0.09, -0.07, \n",
            "s[5,8]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[6,0]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[6,1]: -0.02,  0.05,  0.07, -0.07, \n",
            "s[6,2]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[6,3]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[6,4]: -0.04,  0.05,  0.07, -0.06, \n",
            "s[6,5]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[6,6]: -0.02,  0.05,  0.09, -0.06, \n",
            "s[6,7]: -0.02,  0.05,  0.08, -0.07, \n",
            "s[6,8]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[7,0]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[7,1]: -0.04,  0.06,  0.06, -0.09, \n",
            "s[7,2]: -0.03,  0.05,  0.08, -0.07, \n",
            "s[7,3]: -0.02,  0.04,  0.07, -0.08, \n",
            "s[7,4]: -0.03,  0.03,  0.08, -0.07, \n",
            "s[7,5]: -0.04,  0.04,  0.06, -0.07, \n",
            "s[7,6]: -0.03,  0.07,  0.09, -0.06, \n",
            "s[7,7]: -0.03,  0.05,  0.09, -0.07, \n",
            "s[7,8]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[8,0]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[8,1]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[8,2]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[8,3]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[8,4]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[8,5]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[8,6]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[8,7]:  0.00,  0.00,  0.00,  0.00, \n",
            "s[8,8]:  0.00,  0.00,  0.00,  0.00, \n",
            "프로그램 종료!\n"
          ]
        }
      ]
    }
  ]
}